# Herdan's Law / Heaps' Law
#ðŸ“¥ 
%%
#NLP 
#concept
%%
**Related:**
-  [[Text Normalization]]
-  [[Sentence Boundary Detection]]

---

$|V|=kN^\beta$
- $|V|$ = unique tokens or the number of types. Set of all tokens 
- $N$ = number of total tokens /  independent values
- $k$ param
- $b$ param


Q: Can $b$ be equal to one given that $b$ has been trained in a big big big corpus
No, because in a big corpus there are a lot of repeated words. $|V|$ is the number of unique words, so $b$ would have to be like negative or really small so make the eq. equal. $b$ dominates bc it's bigger