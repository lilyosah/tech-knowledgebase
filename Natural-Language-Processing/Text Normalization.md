# Text Normalization
%%
#NLP 
#concept
%%
**Related:**
-  

---

Unix tool can be used to build quick word count stats for any corpus
==Tokenization:== Parsing corpuses for individual words
==Word normalization:== The task of putting words and tokens into a standard format, choosing a single normal form for words with multiple forms
==Lemmatization:== Determining that two words have the same root despite surface differences 
==Stemming:== A more straightforward way to lower the number of tokens in a corpus where you turn parts of  word into other more simple fragments
**Ex: âœ**  sses -> ss (grasses -> grass)
Sentence segmentation: segmenting text into sentences 